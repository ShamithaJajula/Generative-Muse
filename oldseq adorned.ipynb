{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8lUBMcOvyVNb","executionInfo":{"status":"ok","timestamp":1717749865611,"user_tz":-330,"elapsed":16028,"user":{"displayName":"Shamitha Jajula","userId":"18007923260204706759"}},"outputId":"18c16474-c2d0-464e-a1eb-bc43e85212dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n","WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n","WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n","WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"output_type":"stream","name":"stdout","text":["adorned in shades of red sail sail windy windy windy windy bonds after forsake forsake believe let habitation rain siren worthier surety vermilion vermilion coward coward spread might party party party party snow divine perfect'st perfect'st wrong perfect'st said correspondence life life slow crossed grossly enough ending siege siege bones receipt receipt receipt mind commence men's men's men's men's men's blow blow blow blow bonds bonds wombs refigured spoil spoil spoil spoil dwells interim whom thrust whom curls curls winds felt as different plead plead semblance ending blindness valley men's men's men's men's blow blow blow blow bonds bonds wombs refigured spoil spoil spoil spoil\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout, Bidirectional\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras import utils\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Path to the Shakespeare text file on Google Drive\n","file_path = '/content/drive/My Drive/shakespeare.txt'\n","\n","# Reading the text file\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    text = file.read().splitlines()\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","\n","# Initialize and fit tokenizer\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(text)\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# Create sequences\n","input_sequence = []\n","for line in text:\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequence.append(n_gram_sequence)\n","\n","# Padding sequences to ensure all are the same length\n","input_sequence = pad_sequences(input_sequence, padding='pre')\n","max_len = input_sequence.shape[1]  # Correctly calculating max_len here\n","vocab_size = len(tokenizer.word_index) + 1  # Include +1 for zero padding\n","\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Bidirectional\n","\n","def build_generator(vocab_size, max_len):\n","    model = Sequential([\n","        Embedding(vocab_size, 100, input_length=max_len-1),\n","        Bidirectional(LSTM(128, return_sequences=True)),\n","        Dropout(0.2),\n","        LSTM(128),\n","        Dense(vocab_size, activation='softmax')\n","    ])\n","    return model\n","\n","def build_discriminator(vocab_size, max_len):\n","    model = Sequential([\n","        Embedding(vocab_size, 100, input_length=max_len),\n","        Bidirectional(LSTM(128, return_sequences=True)),\n","        Dropout(0.2),\n","        LSTM(128),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    return model\n","\n","\n","# Now use these variables to build and compile your models\n","generator = build_generator(vocab_size, max_len)\n","discriminator = build_discriminator(vocab_size, max_len)\n","\n","from tensorflow.keras.optimizers import Adam\n","\n","# Compile the discriminator\n","discriminator.compile(optimizer=Adam(lr=0.0002, beta_1=0.5), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Setup and compile the GAN\n","discriminator.trainable = False\n","gan_input = Input(shape=(max_len-1,))\n","gan_output = discriminator(generator(gan_input))\n","gan = Model(gan_input, gan_output)\n","gan.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n","# GAN Model where the discriminator's weights are frozen during generator training\n","discriminator.compile(optimizer=Adam(lr=0.0002, beta_1=0.5), loss='binary_crossentropy', metrics=['accuracy'])\n","discriminator.trainable = False\n","gan_input = Input(shape=(max_len-1,))\n","gan_output = discriminator(generator(gan_input))\n","gan = Model(gan_input, gan_output)\n","gan.compile(optimizer=Adam(lr=0.0002, beta_1=0.5), loss='binary_crossentropy')\n","\n","def train_gan(epochs, batch_size):\n","    for epoch in range(epochs):\n","        idx = np.random.randint(0, input_sequence.shape[0], batch_size)\n","        real_seqs = input_sequence[idx]\n","\n","        noise = np.random.normal(0, 1, (batch_size, max_len-1))\n","        fake_seqs = generator.predict(noise)\n","\n","        d_loss_real = discriminator.train_on_batch(real_seqs, np.ones((batch_size, 1)))\n","        d_loss_fake = discriminator.train_on_batch(fake_seqs, np.zeros((batch_size, 1)))\n","\n","        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Discriminator Loss: {(d_loss_real + d_loss_fake) / 2}, Generator Loss: {g_loss}\")\n","\n","\n","def generate_text(seed_text, next_words=100):\n","    for _ in range(next_words):\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","        token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n","        predicted_probs = generator.predict(token_list, verbose=0)\n","        predicted_index = np.argmax(predicted_probs, axis=-1)[0]\n","        predicted_word = tokenizer.index_word.get(predicted_index, 'unknown')\n","        seed_text += \" \" + predicted_word\n","    return seed_text\n","\n","# Example seed text\n","generated_text = generate_text(\"adorned in shades of red\")\n","print(generated_text)"]},{"cell_type":"code","source":["import tensorflow_probability as tfp"],"metadata":{"id":"st5bZWDQzqwc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### prev\n","def generate_text(seed_text, next_words=100):\n","    for _ in range(next_words):\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","        token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n","        predicted_probs = generator.predict(token_list, verbose=0)\n","        predicted_index = np.argmax(predicted_probs, axis=-1)[0]\n","        predicted_word = tokenizer.index_word.get(predicted_index, 'unknown')\n","        seed_text += \" \" + predicted_word\n","    return seed_text\n","\n","# Example seed text\n","generated_text = generate_text(\"adorned in shades of red\")\n","print(generated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40fM9qpL0aaU","executionInfo":{"status":"ok","timestamp":1717749883375,"user_tz":-330,"elapsed":6153,"user":{"displayName":"Shamitha Jajula","userId":"18007923260204706759"}},"outputId":"19f120ff-4a0c-400d-e6aa-576cba0bda46"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["adorned in shades of red sail sail windy windy windy windy bonds after forsake forsake believe let habitation rain siren worthier surety vermilion vermilion coward coward spread might party party party party snow divine perfect'st perfect'st wrong perfect'st said correspondence life life slow crossed grossly enough ending siege siege bones receipt receipt receipt mind commence men's men's men's men's men's blow blow blow blow bonds bonds wombs refigured spoil spoil spoil spoil dwells interim whom thrust whom curls curls winds felt as different plead plead semblance ending blindness valley men's men's men's men's blow blow blow blow bonds bonds wombs refigured spoil spoil spoil spoil\n"]}]},{"cell_type":"code","source":["#EXP\n","\n","import numpy as np\n","\n","def sample(preds, temperature=1.0):\n","    \"\"\" Helper function to sample an index from a probability array. \"\"\"\n","    preds = np.asarray(preds).astype('float64')\n","    preds = np.log(preds + 1e-10) / temperature  # Adding a small number to avoid log(0)\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probas)\n","\n","def generate_text(seed_text, next_words=100, temperature=0.8):\n","    for _ in range(next_words):\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","        token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n","        predicted_probs = generator.predict(token_list, verbose=0)[0]\n","        next_index = sample(predicted_probs, temperature)  # Use temperature\n","        next_word = tokenizer.index_word.get(next_index, 'unknown')\n","        seed_text += \" \" + next_word\n","    return seed_text\n","\n","# Generate text with a bit of randomness\n","generated_text = generate_text(\"Adorned in shades of red\", next_words=50, temperature=0.8)\n","print(generated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HHocz2JX0hL1","executionInfo":{"status":"ok","timestamp":1717749918349,"user_tz":-330,"elapsed":3134,"user":{"displayName":"Shamitha Jajula","userId":"18007923260204706759"}},"outputId":"f0acb9a9-1f94-419e-8481-2cc10fde3310"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adorned in shades of red presenteth lesson faults rocks chips affairs impiety concealed charter indeed deserts death's parallels adding rolling lover dial's wooing kills measure unfolding heinous divide dressing receiving relief monument dwells soundless dumb bore watching straying swart burn substance spacious temperate convertest answered mad rest ransoms children's vexed possessing acceptable praising costs apparel\n"]}]},{"cell_type":"code","source":["import random\n","\n","def generate_sonnet_line(next_words=10):\n","    # Start with a random initial word from the vocabulary\n","    initial_word = random.choice(list(tokenizer.word_index.keys()))\n","    line = generate_text(initial_word, next_words=next_words)  # Generate the rest of the line\n","    return line\n","\n","def create_sonnet():\n","    # Generate lines for each rhyme scheme\n","    a1 = generate_sonnet_line()\n","    a2 = generate_sonnet_line()\n","    b1 = generate_sonnet_line()\n","    b2 = generate_sonnet_line()\n","    c1 = generate_sonnet_line()\n","    c2 = generate_sonnet_line()\n","    d1 = generate_sonnet_line()\n","    d2 = generate_sonnet_line()\n","    e1 = generate_sonnet_line()\n","    e2 = generate_sonnet_line()\n","    f1 = generate_sonnet_line()\n","    f2 = generate_sonnet_line()\n","    g1 = generate_sonnet_line()\n","    g2 = generate_sonnet_line()\n","\n","    # Concatenate lines according to rhyme scheme\n","    sonnet_structure = \"\\n\".join([\n","        a1, b1, a2, b2,  # ABAB\n","        c1, d1, c2, d2,  # CDCD\n","        e1, f1, e2, f2,  # EFEF\n","        g1, g2  # GG\n","    ])\n","    return sonnet_structure\n","\n","# Example usage to generate a sonnet\n","generated_sonnet = create_sonnet()\n","print(generated_sonnet)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2p_biIpr1Y09","executionInfo":{"status":"ok","timestamp":1717749934517,"user_tz":-330,"elapsed":8753,"user":{"displayName":"Shamitha Jajula","userId":"18007923260204706759"}},"outputId":"0f4f17bd-963f-4636-88f3-42a182e433a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["credit several task forbear acknowledge dare single siege methinks vex is\n","contents composition linger bare ransom defendant hill bark any proudest greatest\n","woe impute buy cry possesseth lascivious lesser churls trees bemoaned faster\n","stern eclipses unstained affords stage jealousy nativity shakespeare ward gav'st votary\n","call foe legacy unto resting straying receiv'st seal skill statues fate\n","chronicle deformed'st neglected seasoned defeat gems several governs single feed'st impregnable\n","termed costly clerk limbecks arise jade bids now head bare mow\n","jewels corrupting wish physic dyed separable sets pricked cruel worthier worthier\n","can deep graced onset presage as fed parts sin interim durst\n","through repay together wherefore presents wife due paying drink determinate unlettered\n","candles sole few already soul receiv'st goodness greet perish sovereign steepy\n","bail slumbers deface outworn travel describe forwards dressed day outworn see\n","curls children's iniquity graves perhaps accident breathe weak respose suff'ring even\n","stormy tenure warm grounded importune none dedicated point record sway'st murd'rous\n"]}]}]}